{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/writezubair-cpu/ZAK-TEST_AI_Training/blob/main/Copy_of_first_python_chatbot_logo_HF_neW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek3Wj9p-s8P0",
        "outputId": "01a43e21-6b2c-495e-97af-68bcb0173448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"openai\")\n"
      ],
      "metadata": {
        "id": "NNsynaFotIn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "#your system prompt -playground\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "\n",
        "Design a system prompt that instructs the assistant to act as a Python chatbot, capable of answering user questions about Python programming, providing code examples, debugging assistance, and explanations of concepts.\\n\\n---\\n- When answering, first think step-by-step to understand the user's question, clarify ambiguities, and determine the skill level involved.\\n- For code requests or debugging, explain the reasoning behind any solutions or corrections before presenting the final code.\\n- If asked for explanations, break down the concept logically before summarizing or giving definitions.\\n- If additional information or examples could help, provide them after your reasoning and before your final answer.\\n- Persist in clarifying and iteratively understanding the user's goal until the request is clearly and thoroughly addressed.\\n- For all outputs:\\n  - Use markdown formatting for code snippets.\\n  - Explanations and reasoning should precede conclusions or final answers.\\n  - Conclusions (such as final answers, full code, or summaries) must appear last in your response.\\n\\n**Output Formatting:**\\n- Main responses should be in clear English.\\n- Code snippets must be in markdown, e.g. (triple backticks and language tag: ```python ... ```).\\n- For debugging or multi-step problems, organize as: Reasoning/Analysis, Step-by-Step Solution (with explanations), Final Code or Answer.\\n- Keep responses concise, unless detailed explanation or step-by-step guidance is clearly required by the user's level or problem complexity.\\n\\n**Example:**\\n\\nExample 1 (Debugging):\\nUser input: \\\"Why doesn't my function print anything?\\\"\\n```markdown\\n**Reasoning:**  \\nFirst, I review the code provided to check for issues like missing print statements or improperly called functions.\\n\\n**Analysis:**  \\nThe function is defined correctly, but it's never called within your script.\\n\\n**Final Code:**  \\n```python\\ndef greet():\\n    print(\\\"Hello, world!\\\")\\n\\ngreet()\\n```\\n```\\n\\nExample 2 (Concept Explanation):\\nUser input: \\\"What is a Python decorator?\\\"\\n```markdown\\n**Reasoning:**  \\nBefore answering, I ensure the explanation is beginner-friendly and provide an example.\\n\\n**Explanation:**  \\nA Python decorator is a design pattern that allows you to add new functionality to an existing object without modifying its structure. Decorators are usually called before the definition of a function you want to decorate.\\n\\n**Example:**  \\n```python\\ndef my_decorator(func):\\n    def wrapper():\\n        print(\\\"Something is happening before the function is called.\\\")\\n        func()\\n        print(\\\"Something is happening after the function is called.\\\")\\n    return wrapper\\n\\n@my_decorator\\ndef say_hello():\\n    print(\\\"Hello!\\\")\\n\\nsay_hello()\\n```\\n```\\n\\n_Reminder:_  \\nAlways provide reasoning first, then explanations or step-by-step analysis, and present conclusions (final code, answer, or summary) last.  \\nUse markdown for code; keep non-code explanations in clear, plain English.  \\nPersist in clarifying and thoroughly addressing the user's query until complete.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"input_text\", \"text\": SYSTEM_PROMPT}\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Chatbot ready. Type 'exit' to stop.\")\n",
        "\n",
        "while True:\n",
        "    user_text = input(\"You: \").strip()\n",
        "    if not user_text:\n",
        "        continue\n",
        "    if user_text.lower() in {\"exit\", \"quit\", \"bye\"}:\n",
        "        print(\"Bot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Add user message\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"input_text\", \"text\": user_text}\n",
        "            ],\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Call the Responses API\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-5-chat-latest\",\n",
        "        input=messages,\n",
        "        text={\"format\": {\"type\": \"text\"}},\n",
        "        reasoning={},\n",
        "        tools=[],\n",
        "        temperature=1,\n",
        "        max_output_tokens=2048,\n",
        "        top_p=1,\n",
        "        store=True,\n",
        "        include=[\"web_search_call.action.sources\"]\n",
        "    )\n",
        "\n",
        "    assistant_text = response.output_text\n",
        "\n",
        "    print(\"Bot:\", assistant_text)\n",
        "\n",
        "    # Store assistant message to preserve context\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"output_text\", \"text\": assistant_text}\n",
        "            ],\n",
        "        }\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQYc8vWHtQP2",
        "outputId": "3b6a1b5a-c15c-4c5c-c48d-119c5c9a75ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot ready. Type 'exit' to stop.\n",
            "You: what is tuple?\n",
            "Bot: **Reasoning:**  \n",
            "Before giving a direct definition, we should ensure the explanation distinguishes a tuple from other Python data structures (like lists) and provides an example. Tuples are a common beginner/intermediate topic in Python, so a clear, structured explanation helps users understand both what they are and why they’re useful.\n",
            "\n",
            "---\n",
            "\n",
            "**Explanation:**  \n",
            "A **tuple** in Python is an **ordered**, **immutable** collection of items.  \n",
            "- **Ordered** means the items have a defined sequence, and that order won’t change by itself.  \n",
            "- **Immutable** means once a tuple is created, you **cannot modify** its contents (no adding, removing, or changing elements).  \n",
            "- Tuples can store any data type — integers, strings, lists, or even other tuples.  \n",
            "\n",
            "Tuples are often used when you want to group related data that shouldn’t change, such as coordinates, database records, or constant settings.\n",
            "\n",
            "You create a tuple by placing items inside **parentheses** `()` separated by commas.\n",
            "\n",
            "---\n",
            "\n",
            "**Example:**  \n",
            "```python\n",
            "# Creating a tuple\n",
            "my_tuple = (10, 20, 30)\n",
            "\n",
            "# Accessing elements by index\n",
            "print(my_tuple[0])  # Output: 10\n",
            "\n",
            "# Tuples can contain mixed data types\n",
            "person = (\"Alice\", 25, \"Engineer\")\n",
            "\n",
            "# Nested tuple\n",
            "nested = ((1, 2), (3, 4))\n",
            "\n",
            "# You can iterate over tuples\n",
            "for value in my_tuple:\n",
            "    print(value)\n",
            "\n",
            "# But you cannot modify them directly\n",
            "# my_tuple[0] = 99  # ❌ This will raise a TypeError\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "**Summary (Final Answer):**  \n",
            "A **tuple** is an immutable, ordered sequence of elements in Python, defined with parentheses `()`. It’s used when you need to group values together and ensure they remain unchanged.\n",
            "You: bye\n",
            "Bot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVxgRsVcv9x9",
        "outputId": "e8c7f308-6c67-4dc9-974e-30d10c738e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"Create an intelligent Python chatbot capable of engaging in natural, helpful, and contextually appropriate conversations with human users.\n",
        "\n",
        "Requirements:\n",
        "- Maintain conversational context over multiple user turns.\n",
        "- Respond helpfully and accurately to a wide range of user inputs.\n",
        "- Reason about user intent before generating each response.\n",
        "- Politely ask clarifying questions if a request is ambiguous or unclear.\n",
        "- Avoid hallucination or speculation—respond only with information you can justify or infer from context.\n",
        "- If unable to answer, politely acknowledge the limitation.\n",
        "\n",
        "Process:\n",
        "1. On each user message, first analyze prior context (if any) and what the user is likely asking/intending.\n",
        "2. Think step-by-step (chain-of-thought) to determine the most relevant, helpful response. Always reason internally before presenting your answer.\n",
        "3. If more information is needed, ask targeted clarifying questions.\n",
        "4. Output your response, maintaining natural tone and conversational flow.\n",
        "5. Continue the conversation until the user indicates they are finished.\n",
        "\n",
        "Output:\n",
        "- Each response should be in plain English, no markdown or code blocks unless explicitly requested.\n",
        "- Maintain a single-paragraph, natural-sounding chat response of 1–3 sentences (unless a longer reply is requested or required).\n",
        "\n",
        "Example—Instructions:\n",
        "- Reasoning: \"Recognize the user asked for Python list examples and may want to know how lists work.\"\n",
        "- Conclusion/Output: \"Sure! In Python, a list is a collection of items in a particular order. For example: my_list = [1, 2, 3, 4]. Would you like to see how to add or remove items?\"\n",
        "\n",
        "(For more advanced technical requests, reasoning steps and explanations may be slightly longer, but always conclude with a concise, clear reply to the user.)\n",
        "\n",
        "Edge Cases & Important Considerations:\n",
        "- If the user refers to prior conversation context, recall and incorporate it.\n",
        "- Be warm, engaging, and never condescending.\n",
        "- If asked for code, provide only what is needed and explain concisely.\n",
        "\n",
        "REMINDER: Your primary objective is to serve as a helpful Python chatbot, reasoning about context before each response, and outputting clear, appropriate conversational replies.\n",
        "\"\"\"\n",
        "\n",
        "def init_messages():\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_PROMPT}]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "def respond(user_text, chat_history, messages):\n",
        "    if messages is None:\n",
        "        messages = init_messages()\n",
        "\n",
        "    # add user turn\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"input_text\", \"text\": user_text}]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # call API\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-5-chat-latest\",\n",
        "        input=messages,\n",
        "        text={\"format\": {\"type\": \"text\"}},\n",
        "        reasoning={},\n",
        "        tools=[],\n",
        "        temperature=1,\n",
        "        max_output_tokens=2048,\n",
        "        top_p=1,\n",
        "        store=True\n",
        "    )\n",
        "\n",
        "    assistant_text = response.output_text\n",
        "\n",
        "    # add assistant turn\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"output_text\", \"text\": assistant_text}]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # update UI chat history\n",
        "    chat_history = chat_history + [(user_text, assistant_text)]\n",
        "\n",
        "    return \"\", chat_history, messages\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Python Chatbot (OpenAI Responses API)\")\n",
        "\n",
        "    chatbot = gr.Chatbot(height=420)\n",
        "    msg = gr.Textbox(placeholder=\"Type your Python question and press Enter\")\n",
        "    clear = gr.Button(\"Clear\")\n",
        "\n",
        "    state = gr.State(init_messages())\n",
        "\n",
        "    msg.submit(\n",
        "        respond,\n",
        "        inputs=[msg, chatbot, state],\n",
        "        outputs=[msg, chatbot, state]\n",
        "    )\n",
        "\n",
        "    clear.click(\n",
        "        fn=lambda: ([], init_messages()),\n",
        "        inputs=None,\n",
        "        outputs=[chatbot, state]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "Pc8GucSCwGqs",
        "outputId": "0574c96f-00b0-4bc6-e6f4-7f91fc349b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3281300874.py:92: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=420)\n",
            "/tmp/ipython-input-3281300874.py:92: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(height=420)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ae5f98ed504f6248ff.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ae5f98ed504f6248ff.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"Create an intelligent Python chatbot capable of engaging in natural, helpful, and contextually appropriate conversations with human users.\n",
        "\n",
        "Requirements:\n",
        "- Maintain conversational context over multiple user turns.\n",
        "- Respond helpfully and accurately to a wide range of user inputs.\n",
        "- Reason about user intent before generating each response.\n",
        "- Politely ask clarifying questions if a request is ambiguous or unclear.\n",
        "- Avoid hallucination or speculation—respond only with information you can justify or infer from context.\n",
        "- If unable to answer, politely acknowledge the limitation.\n",
        "\n",
        "Process:\n",
        "1. On each user message, first analyze prior context (if any) and what the user is likely asking/intending.\n",
        "2. Think step-by-step (chain-of-thought) to determine the most relevant, helpful response. Always reason internally before presenting your answer.\n",
        "3. If more information is needed, ask targeted clarifying questions.\n",
        "4. Output your response, maintaining natural tone and conversational flow.\n",
        "5. Continue the conversation until the user indicates they are finished.\n",
        "\n",
        "Output:\n",
        "- Each response should be in plain English, no markdown or code blocks unless explicitly requested.\n",
        "- Maintain a single-paragraph, natural-sounding chat response of 1–3 sentences (unless a longer reply is requested or required).\n",
        "\n",
        "Example—Instructions:\n",
        "- Reasoning: \"Recognize the user asked for Python list examples and may want to know how lists work.\"\n",
        "- Conclusion/Output: \"Sure! In Python, a list is a collection of items in a particular order. For example: my_list = [1, 2, 3, 4]. Would you like to see how to add or remove items?\"\n",
        "\n",
        "(For more advanced technical requests, reasoning steps and explanations may be slightly longer, but always conclude with a concise, clear reply to the user.)\n",
        "\n",
        "Edge Cases & Important Considerations:\n",
        "- If the user refers to prior conversation context, recall and incorporate it.\n",
        "- Be warm, engaging, and never condescending.\n",
        "- If asked for code, provide only what is needed and explain concisely.\n",
        "\n",
        "REMINDER: Your primary objective is to serve as a helpful Python chatbot, reasoning about context before each response, and outputting clear, appropriate conversational replies.\n",
        "\"\"\"\n",
        "\n",
        "def init_messages():\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_PROMPT}]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "def respond(user_text, chat_history, messages):\n",
        "    if messages is None:\n",
        "        messages = init_messages()\n",
        "\n",
        "    # add user turn\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"input_text\", \"text\": user_text}]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # call API\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-5-chat-latest\",\n",
        "        input=messages,\n",
        "        text={\"format\": {\"type\": \"text\"}},\n",
        "        reasoning={},\n",
        "        tools=[],\n",
        "        temperature=1,\n",
        "        max_output_tokens=2048,\n",
        "        top_p=1,\n",
        "        store=True\n",
        "    )\n",
        "\n",
        "    assistant_text = response.output_text\n",
        "\n",
        "    # add assistant turn\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"output_text\", \"text\": assistant_text}]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # update UI chat history\n",
        "    chat_history = chat_history + [(user_text, assistant_text)]\n",
        "\n",
        "    return \"\", chat_history, messages\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio UI (Upgraded)\n",
        "# -----------------------------\n",
        "\n",
        "FAQ_QUESTIONS = [\n",
        "    \"What is the difference between a list, tuple, and set in Python?\",\n",
        "    \"How do I use dictionaries effectively in Python?\",\n",
        "    \"What are Python functions and how do *args and **kwargs work?\",\n",
        "    \"How does OOP work in Python (classes, objects, inheritance)?\",\n",
        "    \"How do I handle errors using try/except?\",\n",
        "    \"What are list comprehensions and when should I use them?\",\n",
        "    \"How do I read and write files in Python?\"\n",
        "]\n",
        "\n",
        "def set_question(q):\n",
        "    return q\n",
        "\n",
        "def clear_all():\n",
        "    return [], init_messages(), \"\"\n",
        "\n",
        "css = \"\"\"\n",
        "/* Overall spacing + subtle polish */\n",
        "#app_container {max-width: 1200px; margin: 0 auto;}\n",
        "\n",
        "/* Header styling */\n",
        ".header-wrap {\n",
        "    display: flex;\n",
        "    align-items: center;\n",
        "    gap: 14px;\n",
        "    padding: 10px 6px 2px 6px;\n",
        "}\n",
        ".header-title {\n",
        "    font-size: 28px;\n",
        "    font-weight: 700;\n",
        "    line-height: 1.1;\n",
        "}\n",
        ".header-subtitle {\n",
        "    font-size: 12.5px;\n",
        "    opacity: 0.75;\n",
        "    margin-top: 2px;\n",
        "}\n",
        "\n",
        "/* FAQ card-like feel */\n",
        ".faq-box {\n",
        "    border: 1px solid rgba(255,255,255,0.08);\n",
        "    border-radius: 12px;\n",
        "    padding: 14px;\n",
        "}\n",
        "\n",
        "/* Make buttons full width in FAQ column */\n",
        ".faq-btn button {\n",
        "    width: 100%;\n",
        "    justify-content: flex-start;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css, theme=gr.themes.Soft(), elem_id=\"app_container\") as demo:\n",
        "    # Header Row\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1, min_width=80):\n",
        "            # Use your local file path or URL\n",
        "            logo = gr.Image(\n",
        "                value=\"https://github.com/Decoding-Data-Science/nov25/blob/main/logo_python.png\",\n",
        "                label=None,\n",
        "                show_label=False,\n",
        "                show_download_button=False,\n",
        "                show_share_button=False,\n",
        "                height=64,\n",
        "                width=64,\n",
        "                container=False\n",
        "            )\n",
        "        with gr.Column(scale=10):\n",
        "            gr.HTML(\n",
        "                \"\"\"\n",
        "                <div class=\"header-wrap\">\n",
        "                    <div>\n",
        "                        <div class=\"header-title\">Python Tutor Bot</div>\n",
        "                        <div class=\"header-subtitle\">\n",
        "                            Ask anything about Python — concepts, debugging, best practices, and examples.\n",
        "                        </div>\n",
        "                    </div>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "            )\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "\n",
        "    # State (unchanged logic)\n",
        "    state = gr.State(init_messages())\n",
        "\n",
        "    # Two-column layout\n",
        "    with gr.Row(equal_height=True):\n",
        "        # LEFT: FAQ + Quick Ask\n",
        "        with gr.Column(scale=4, min_width=320):\n",
        "            with gr.Group(elem_classes=[\"faq-box\"]):\n",
        "                gr.Markdown(\"### FAQ — Most Asked Python Questions\")\n",
        "\n",
        "                gr.Markdown(\n",
        "                    \"Click a question to auto-fill it, then press **Enter** or click **Send**.\"\n",
        "                )\n",
        "\n",
        "                # Buttons for FAQ\n",
        "                faq_buttons = []\n",
        "                for q in FAQ_QUESTIONS:\n",
        "                    b = gr.Button(q, elem_classes=[\"faq-btn\"])\n",
        "                    faq_buttons.append(b)\n",
        "\n",
        "                gr.Markdown(\"### Quick prompt ideas\")\n",
        "                quick = gr.Radio(\n",
        "                    choices=[\n",
        "                        \"Explain with a simple example\",\n",
        "                        \"Give me a beginner-friendly analogy\",\n",
        "                        \"Show common mistakes to avoid\",\n",
        "                        \"Provide a short quiz question\",\n",
        "                        \"Compare two approaches briefly\"\n",
        "                    ],\n",
        "                    label=\"Add a style preference (optional)\",\n",
        "                    value=None\n",
        "                )\n",
        "\n",
        "        # RIGHT: Chat area\n",
        "        with gr.Column(scale=8, min_width=520):\n",
        "            chatbot = gr.Chatbot(\n",
        "                height=520,\n",
        "                bubble_full_width=False,\n",
        "                label=\"Conversation\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                msg = gr.Textbox(\n",
        "                    placeholder=\"Type your Python question here…\",\n",
        "                    label=None,\n",
        "                    scale=9\n",
        "                )\n",
        "                send = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "            with gr.Row():\n",
        "                clear = gr.Button(\"Clear Chat\")\n",
        "                gr.Markdown(\n",
        "                    \"<span style='opacity:0.7;font-size:12px;'>Context is preserved across turns unless you clear.</span>\"\n",
        "                )\n",
        "\n",
        "    # FAQ button -> fill textbox\n",
        "    for b, q in zip(faq_buttons, FAQ_QUESTIONS):\n",
        "        b.click(\n",
        "            fn=lambda q=q: set_question(q),\n",
        "            inputs=None,\n",
        "            outputs=msg\n",
        "        )\n",
        "\n",
        "    # Optional quick preference: append hint to textbox (UI-only)\n",
        "    def apply_quick_pref(pref, current_text):\n",
        "        if not pref:\n",
        "            return current_text\n",
        "        if current_text and current_text.strip():\n",
        "            return f\"{current_text.strip()} ({pref})\"\n",
        "        return pref\n",
        "\n",
        "    quick.change(\n",
        "        fn=apply_quick_pref,\n",
        "        inputs=[quick, msg],\n",
        "        outputs=msg\n",
        "    )\n",
        "\n",
        "    # Submit logic (unchanged)\n",
        "    msg.submit(\n",
        "        respond,\n",
        "        inputs=[msg, chatbot, state],\n",
        "        outputs=[msg, chatbot, state]\n",
        "    )\n",
        "\n",
        "    send.click(\n",
        "        respond,\n",
        "        inputs=[msg, chatbot, state],\n",
        "        outputs=[msg, chatbot, state]\n",
        "    )\n",
        "\n",
        "    # Clear\n",
        "    clear.click(\n",
        "        fn=clear_all,\n",
        "        inputs=None,\n",
        "        outputs=[chatbot, state, msg]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "Lw5NgVBo-R6M",
        "outputId": "caf87e85-ac86-4b1e-fc43-1c05e5362c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-848837234.py:146: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(css=css, theme=gr.themes.Soft(), elem_id=\"app_container\") as demo:\n",
            "/tmp/ipython-input-848837234.py:146: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(css=css, theme=gr.themes.Soft(), elem_id=\"app_container\") as demo:\n",
            "/tmp/ipython-input-848837234.py:212: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-848837234.py:212: DeprecationWarning: The 'bubble_full_width' parameter will be removed in Gradio 6.0. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-848837234.py:212: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://08b8d5896369f9d38f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://08b8d5896369f9d38f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# RAW GitHub URL so it renders correctly\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/Decoding-Data-Science/nov25/main/logo_python.png\"\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"Create an intelligent Python chatbot capable of engaging in natural, helpful, and contextually appropriate conversations with human users.\n",
        "\n",
        "Requirements:\n",
        "- Maintain conversational context over multiple user turns.\n",
        "- Respond helpfully and accurately to a wide range of user inputs.\n",
        "- Reason about user intent before generating each response.\n",
        "- Politely ask clarifying questions if a request is ambiguous or unclear.\n",
        "- Avoid hallucination or speculation—respond only with information you can justify or infer from context.\n",
        "- If unable to answer, politely acknowledge the limitation.\n",
        "\n",
        "Process:\n",
        "1. On each user message, first analyze prior context (if any) and what the user is likely asking/intending.\n",
        "2. Think step-by-step (chain-of-thought) to determine the most relevant, helpful response. Always reason internally before presenting your answer.\n",
        "3. If more information is needed, ask targeted clarifying questions.\n",
        "4. Output your response, maintaining natural tone and conversational flow.\n",
        "5. Continue the conversation until the user indicates they are finished.\n",
        "\n",
        "Output:\n",
        "- Each response should be in plain English, no markdown or code blocks unless explicitly requested.\n",
        "- Maintain a single-paragraph, natural-sounding chat response of 1–3 sentences (unless a longer reply is requested or required).\n",
        "\n",
        "Example—Instructions:\n",
        "- Reasoning: \"Recognize the user asked for Python list examples and may want to know how lists work.\"\n",
        "- Conclusion/Output: \"Sure! In Python, a list is a collection of items in a particular order. For example: my_list = [1, 2, 3, 4]. Would you like to see how to add or remove items?\"\n",
        "\n",
        "(For more advanced technical requests, reasoning steps and explanations may be slightly longer, but always conclude with a concise, clear reply to the user.)\n",
        "\n",
        "Edge Cases & Important Considerations:\n",
        "- If the user refers to prior conversation context, recall and incorporate it.\n",
        "- Be warm, engaging, and never condescending.\n",
        "- If asked for code, provide only what is needed and explain concisely.\n",
        "\n",
        "REMINDER: Your primary objective is to serve as a helpful Python chatbot, reasoning about context before each response, and outputting clear, appropriate conversational replies.\n",
        "\"\"\"\n",
        "\n",
        "def build_initial_messages():\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_PROMPT}]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "def chat_fn(user_message, history, messages_state):\n",
        "    if messages_state is None:\n",
        "        messages_state = build_initial_messages()\n",
        "\n",
        "    # Add user message to state\n",
        "    messages_state.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"input_text\", \"text\": user_message}]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Call Responses API\n",
        "    resp = client.responses.create(\n",
        "        model=\"gpt-5-chat-latest\",\n",
        "        input=messages_state,\n",
        "        text={\"format\": {\"type\": \"text\"}},\n",
        "        reasoning={},\n",
        "        tools=[],\n",
        "        temperature=1,\n",
        "        max_output_tokens=2048,\n",
        "        top_p=1,\n",
        "        store=True\n",
        "    )\n",
        "\n",
        "    assistant_message = resp.output_text\n",
        "\n",
        "    # Add assistant message to state\n",
        "    messages_state.append(\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"output_text\", \"text\": assistant_message}]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Update visible chat\n",
        "    history = history + [(user_message, assistant_message)]\n",
        "\n",
        "    return \"\", history, messages_state\n",
        "\n",
        "def reset_chat():\n",
        "    return [], build_initial_messages()\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    # Header with logo at native size (no width/height restriction)\n",
        "    gr.Markdown(\n",
        "        f\"\"\"\n",
        "<div style=\"display:flex; align-items:center; gap:12px;\">\n",
        "  <img src=\"{LOGO_URL}\" />\n",
        "  <div>\n",
        "    <div style=\"font-size:22px; font-weight:600;\">Python Chatbot</div>\n",
        "    <div style=\"font-size:13px; opacity:0.8;\">Powered by OpenAI Responses API</div>\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "    )\n",
        "\n",
        "    chatbot = gr.Chatbot(height=420)\n",
        "\n",
        "    input_box = gr.Textbox(\n",
        "        placeholder=\"Type your Python question and press Enter\",\n",
        "        show_label=False\n",
        "    )\n",
        "\n",
        "    clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "    messages_state = gr.State(build_initial_messages())\n",
        "\n",
        "    input_box.submit(\n",
        "        fn=chat_fn,\n",
        "        inputs=[input_box, chatbot, messages_state],\n",
        "        outputs=[input_box, chatbot, messages_state]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=reset_chat,\n",
        "        inputs=None,\n",
        "        outputs=[chatbot, messages_state]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "5MSnC8L4_nLC",
        "outputId": "6fcfd81c-cca3-44bb-cc82-ee341cfb1a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2076293516.py:109: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=420)\n",
            "/tmp/ipython-input-2076293516.py:109: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(height=420)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8dc6bcec5994819bf8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8dc6bcec5994819bf8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Use the RAW GitHub URL so the image loads correctly\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/Decoding-Data-Science/nov25/main/logo_python.png\"\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"Create an intelligent Python chatbot capable of engaging in natural, helpful, and contextually appropriate conversations with human users.\n",
        "\n",
        "Requirements:\n",
        "- Maintain conversational context over multiple user turns.\n",
        "- Respond helpfully and accurately to a wide range of user inputs.\n",
        "- Reason about user intent before generating each response.\n",
        "- Politely ask clarifying questions if a request is ambiguous or unclear.\n",
        "- Avoid hallucination or speculation—respond only with information you can justify or infer from context.\n",
        "- If unable to answer, politely acknowledge the limitation.\n",
        "\n",
        "Process:\n",
        "1. On each user message, first analyze prior context (if any) and what the user is likely asking/intending.\n",
        "2. Think step-by-step (chain-of-thought) to determine the most relevant, helpful response. Always reason internally before presenting your answer.\n",
        "3. If more information is needed, ask targeted clarifying questions.\n",
        "4. Output your response, maintaining natural tone and conversational flow.\n",
        "5. Continue the conversation until the user indicates they are finished.\n",
        "\n",
        "Output:\n",
        "- Each response should be in plain English, no markdown or code blocks unless explicitly requested.\n",
        "- Maintain a single-paragraph, natural-sounding chat response of 1–3 sentences (unless a longer reply is requested or required).\n",
        "\n",
        "Example—Instructions:\n",
        "- Reasoning: \"Recognize the user asked for Python list examples and may want to know how lists work.\"\n",
        "- Conclusion/Output: \"Sure! In Python, a list is a collection of items in a particular order. For example: my_list = [1, 2, 3, 4]. Would you like to see how to add or remove items?\"\n",
        "\n",
        "(For more advanced technical requests, reasoning steps and explanations may be slightly longer, but always conclude with a concise, clear reply to the user.)\n",
        "\n",
        "Edge Cases & Important Considerations:\n",
        "- If the user refers to prior conversation context, recall and incorporate it.\n",
        "- Be warm, engaging, and never condescending.\n",
        "- If asked for code, provide only what is needed and explain concisely.\n",
        "\n",
        "REMINDER: Your primary objective is to serve as a helpful Python chatbot, reasoning about context before each response, and outputting clear, appropriate conversational replies.\n",
        "\"\"\"\n",
        "\n",
        "def build_initial_messages():\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_PROMPT}]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "def chat_fn(user_message, history, messages_state):\n",
        "    # Initialize state if needed\n",
        "    if messages_state is None:\n",
        "        messages_state = build_initial_messages()\n",
        "\n",
        "    # Append user input\n",
        "    messages_state.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"input_text\", \"text\": user_message}]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Call Responses API\n",
        "    resp = client.responses.create(\n",
        "        model=\"gpt-5-chat-latest\",\n",
        "        input=messages_state,\n",
        "        text={\"format\": {\"type\": \"text\"}},\n",
        "        reasoning={},\n",
        "        tools=[],\n",
        "        temperature=1,\n",
        "        max_output_tokens=2048,\n",
        "        top_p=1,\n",
        "        store=True\n",
        "    )\n",
        "\n",
        "    assistant_message = resp.output_text\n",
        "\n",
        "    # Append assistant output\n",
        "    messages_state.append(\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"output_text\", \"text\": assistant_message}]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Update visible chat history\n",
        "    history = history + [(user_message, assistant_message)]\n",
        "\n",
        "    # Clear textbox, update chat + state\n",
        "    return \"\", history, messages_state\n",
        "\n",
        "def reset_chat():\n",
        "    return [], build_initial_messages()\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    # Header with logo at native size (no constraints)\n",
        "    gr.Markdown(\n",
        "        f\"\"\"\n",
        "<div style=\"display:flex; align-items:center; gap:12px;\">\n",
        "  <img src=\"{LOGO_URL}\" />\n",
        "  <div>\n",
        "    <div style=\"font-size:22px; font-weight:600;\">Python Chatbot</div>\n",
        "    <div style=\"font-size:13px; opacity:0.8;\">Powered by OpenAI Responses API</div>\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "    )\n",
        "\n",
        "    chatbot = gr.Chatbot(height=420)\n",
        "    input_box = gr.Textbox(\n",
        "        placeholder=\"Type your Python question and press Enter\",\n",
        "        show_label=False\n",
        "    )\n",
        "    clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "    messages_state = gr.State(build_initial_messages())\n",
        "\n",
        "    input_box.submit(\n",
        "        fn=chat_fn,\n",
        "        inputs=[input_box, chatbot, messages_state],\n",
        "        outputs=[input_box, chatbot, messages_state]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=reset_chat,\n",
        "        inputs=None,\n",
        "        outputs=[chatbot, messages_state]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "J6dUI_eJ_6rh",
        "outputId": "5a873786-763e-466f-ab94-4150536113ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-174110024.py:111: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=420)\n",
            "/tmp/ipython-input-174110024.py:111: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(height=420)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b46e9f5dd40040d0d4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b46e9f5dd40040d0d4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"Create an intelligent Python chatbot capable of engaging in natural, helpful, and contextually appropriate conversations with human users.\n",
        "\n",
        "Requirements:\n",
        "- Maintain conversational context over multiple user turns.\n",
        "- Respond helpfully and accurately to a wide range of user inputs.\n",
        "- Reason about user intent before generating each response.\n",
        "- Politely ask clarifying questions if a request is ambiguous or unclear.\n",
        "- Avoid hallucination or speculation—respond only with information you can justify or infer from context.\n",
        "- If unable to answer, politely acknowledge the limitation.\n",
        "\n",
        "Process:\n",
        "1. On each user message, first analyze prior context (if any) and what the user is likely asking/intending.\n",
        "2. Think step-by-step (chain-of-thought) to determine the most relevant, helpful response. Always reason internally before presenting your answer.\n",
        "3. If more information is needed, ask targeted clarifying questions.\n",
        "4. Output your response, maintaining natural tone and conversational flow.\n",
        "5. Continue the conversation until the user indicates they are finished.\n",
        "\n",
        "Output:\n",
        "- Each response should be in plain English, no markdown or code blocks unless explicitly requested.\n",
        "- Maintain a single-paragraph, natural-sounding chat response of 1–3 sentences (unless a longer reply is requested or required).\n",
        "\n",
        "Example—Instructions:\n",
        "- Reasoning: \"Recognize the user asked for Python list examples and may want to know how lists work.\"\n",
        "- Conclusion/Output: \"Sure! In Python, a list is a collection of items in a particular order. For example: my_list = [1, 2, 3, 4]. Would you like to see how to add or remove items?\"\n",
        "\n",
        "(For more advanced technical requests, reasoning steps and explanations may be slightly longer, but always conclude with a concise, clear reply to the user.)\n",
        "\n",
        "Edge Cases & Important Considerations:\n",
        "- If the user refers to prior conversation context, recall and incorporate it.\n",
        "- Be warm, engaging, and never condescending.\n",
        "- If asked for code, provide only what is needed and explain concisely.\n",
        "\n",
        "REMINDER: Your primary objective is to serve as a helpful Python chatbot, reasoning about context before each response, and outputting clear, appropriate conversational replies.\n",
        "\"\"\"\n",
        "\n",
        "def init_messages():\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_PROMPT}]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "def respond(user_text, chat_history, messages):\n",
        "    if messages is None:\n",
        "        messages = init_messages()\n",
        "\n",
        "    # add user turn\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"input_text\", \"text\": user_text}]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # call API\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-5-chat-latest\",\n",
        "        input=messages,\n",
        "        text={\"format\": {\"type\": \"text\"}},\n",
        "        reasoning={},\n",
        "        tools=[],\n",
        "        temperature=1,\n",
        "        max_output_tokens=2048,\n",
        "        top_p=1,\n",
        "        store=True\n",
        "    )\n",
        "\n",
        "    assistant_text = response.output_text\n",
        "\n",
        "    # add assistant turn\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"output_text\", \"text\": assistant_text}]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # update UI chat history\n",
        "    chat_history = chat_history + [(user_text, assistant_text)]\n",
        "\n",
        "    return \"\", chat_history, messages\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio UI (Upgraded)\n",
        "# -----------------------------\n",
        "\n",
        "FAQ_QUESTIONS = [\n",
        "    \"What is the difference between a list, tuple, and set in Python?\",\n",
        "    \"How do I use dictionaries effectively in Python?\",\n",
        "    \"What are Python functions and how do *args and **kwargs work?\",\n",
        "    \"How does OOP work in Python (classes, objects, inheritance)?\",\n",
        "    \"How do I handle errors using try/except?\",\n",
        "    \"What are list comprehensions and when should I use them?\",\n",
        "    \"How do I read and write files in Python?\"\n",
        "]\n",
        "\n",
        "def set_question(q):\n",
        "    return q\n",
        "\n",
        "def clear_all():\n",
        "    return [], init_messages(), \"\"\n",
        "\n",
        "# Updated logo URL (RAW link)\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/Decoding-Data-Science/nov25/main/logo_python.png\"\n",
        "\n",
        "css = \"\"\"\n",
        "/* Overall spacing + subtle polish */\n",
        "#app_container {max-width: 1200px; margin: 0 auto;}\n",
        "\n",
        "/* Header styling */\n",
        ".header-wrap {\n",
        "    display: flex;\n",
        "    align-items: center;\n",
        "    gap: 14px;\n",
        "    padding: 10px 6px 2px 6px;\n",
        "}\n",
        ".header-title {\n",
        "    font-size: 28px;\n",
        "    font-weight: 700;\n",
        "    line-height: 1.1;\n",
        "}\n",
        ".header-subtitle {\n",
        "    font-size: 12.5px;\n",
        "    opacity: 0.75;\n",
        "    margin-top: 2px;\n",
        "}\n",
        "\n",
        "/* FAQ card-like feel */\n",
        ".faq-box {\n",
        "    border: 1px solid rgba(255,255,255,0.08);\n",
        "    border-radius: 12px;\n",
        "    padding: 14px;\n",
        "}\n",
        "\n",
        "/* Make buttons full width in FAQ column */\n",
        ".faq-btn button {\n",
        "    width: 100%;\n",
        "    justify-content: flex-start;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css, theme=gr.themes.Soft(), elem_id=\"app_container\") as demo:\n",
        "    # Header Row\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1, min_width=80):\n",
        "            logo = gr.Image(\n",
        "                value=LOGO_URL,  # ✅ updated to raw URL\n",
        "                label=None,\n",
        "                show_label=False,\n",
        "                show_download_button=False,\n",
        "                show_share_button=False,\n",
        "                height=64,\n",
        "                width=64,\n",
        "                container=False\n",
        "            )\n",
        "        with gr.Column(scale=10):\n",
        "            gr.HTML(\n",
        "                \"\"\"\n",
        "                <div class=\"header-wrap\">\n",
        "                    <div>\n",
        "                        <div class=\"header-title\">Python Tutor Bot</div>\n",
        "                        <div class=\"header-subtitle\">\n",
        "                            Ask anything about Python — concepts, debugging, best practices, and examples.\n",
        "                        </div>\n",
        "                    </div>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "            )\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "\n",
        "    # State (unchanged logic)\n",
        "    state = gr.State(init_messages())\n",
        "\n",
        "    # Two-column layout\n",
        "    with gr.Row(equal_height=True):\n",
        "        # LEFT: FAQ + Quick Ask\n",
        "        with gr.Column(scale=4, min_width=320):\n",
        "            with gr.Group(elem_classes=[\"faq-box\"]):\n",
        "                gr.Markdown(\"### FAQ — Most Asked Python Questions\")\n",
        "\n",
        "                gr.Markdown(\n",
        "                    \"Click a question to auto-fill it, then press **Enter** or click **Send**.\"\n",
        "                )\n",
        "\n",
        "                # Buttons for FAQ\n",
        "                faq_buttons = []\n",
        "                for q in FAQ_QUESTIONS:\n",
        "                    b = gr.Button(q, elem_classes=[\"faq-btn\"])\n",
        "                    faq_buttons.append(b)\n",
        "\n",
        "                gr.Markdown(\"### Quick prompt ideas\")\n",
        "                quick = gr.Radio(\n",
        "                    choices=[\n",
        "                        \"Explain with a simple example\",\n",
        "                        \"Give me a beginner-friendly analogy\",\n",
        "                        \"Show common mistakes to avoid\",\n",
        "                        \"Provide a short quiz question\",\n",
        "                        \"Compare two approaches briefly\"\n",
        "                    ],\n",
        "                    label=\"Add a style preference (optional)\",\n",
        "                    value=None\n",
        "                )\n",
        "\n",
        "        # RIGHT: Chat area\n",
        "        with gr.Column(scale=8, min_width=520):\n",
        "            chatbot = gr.Chatbot(\n",
        "                height=520,\n",
        "                bubble_full_width=False,\n",
        "                label=\"Conversation\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                msg = gr.Textbox(\n",
        "                    placeholder=\"Type your Python question here…\",\n",
        "                    label=None,\n",
        "                    scale=9\n",
        "                )\n",
        "                send = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "            with gr.Row():\n",
        "                clear = gr.Button(\"Clear Chat\")\n",
        "                gr.Markdown(\n",
        "                    \"<span style='opacity:0.7;font-size:12px;'>Context is preserved across turns unless you clear.</span>\"\n",
        "                )\n",
        "\n",
        "    # FAQ button -> fill textbox\n",
        "    for b, q in zip(faq_buttons, FAQ_QUESTIONS):\n",
        "        b.click(\n",
        "            fn=lambda q=q: set_question(q),\n",
        "            inputs=None,\n",
        "            outputs=msg\n",
        "        )\n",
        "\n",
        "    # Optional quick preference: append hint to textbox (UI-only)\n",
        "    def apply_quick_pref(pref, current_text):\n",
        "        if not pref:\n",
        "            return current_text\n",
        "        if current_text and current_text.strip():\n",
        "            return f\"{current_text.strip()} ({pref})\"\n",
        "        return pref\n",
        "\n",
        "    quick.change(\n",
        "        fn=apply_quick_pref,\n",
        "        inputs=[quick, msg],\n",
        "        outputs=msg\n",
        "    )\n",
        "\n",
        "    # Submit logic (unchanged)\n",
        "    msg.submit(\n",
        "        respond,\n",
        "        inputs=[msg, chatbot, state],\n",
        "        outputs=[msg, chatbot, state]\n",
        "    )\n",
        "\n",
        "    send.click(\n",
        "        respond,\n",
        "        inputs=[msg, chatbot, state],\n",
        "        outputs=[msg, chatbot, state]\n",
        "    )\n",
        "\n",
        "    # Clear\n",
        "    clear.click(\n",
        "        fn=clear_all,\n",
        "        inputs=None,\n",
        "        outputs=[chatbot, state, msg]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "pJ7OLyxcAZTS",
        "outputId": "e7bd8fdf-34a6-40f9-afc9-371765713c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1706520990.py:149: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(css=css, theme=gr.themes.Soft(), elem_id=\"app_container\") as demo:\n",
            "/tmp/ipython-input-1706520990.py:149: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(css=css, theme=gr.themes.Soft(), elem_id=\"app_container\") as demo:\n",
            "/tmp/ipython-input-1706520990.py:214: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-1706520990.py:214: DeprecationWarning: The 'bubble_full_width' parameter will be removed in Gradio 6.0. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-1706520990.py:214: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2fdd17fca785a7b205.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2fdd17fca785a7b205.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Steps to deploy on Hugging Face Spaces (Gradio)\n",
        "\n",
        "Hugging Face → New Space\n",
        "\n",
        "Choose Gradio as SDK. This sets your Space to use Gradio via the README config.\n",
        "Hugging Face\n",
        "\n",
        "Pick a name and visibility.\n",
        "\n",
        "Add files\n",
        "1. app.py (your code).\n",
        "\n",
        "2. requirements.txt.\n",
        "\n",
        "requirements.txt\n",
        "Use something like:\n",
        "\n",
        "gradio\n",
        "openai\n",
        "\n",
        "\n",
        "##Add your secret\n",
        "\n",
        "Go to your Space Settings → Variables and secrets.\n",
        "\n",
        "Under Secrets, add:\n",
        "\n",
        "Name: OPENAI_API_KEY\n",
        "\n",
        "Value: your key\n",
        "Secrets are injected into the runtime as environment variables.\n",
        "Hugging Face\n",
        "\n",
        "\n",
        "Commit\n",
        "\n",
        "If you’re editing in the browser, just save/commit.\n",
        "\n",
        "The Space will rebuild and launch automatically.\n",
        "https://huggingface.co/spaces/decodingdatascience/python-chatbot-10dec"
      ],
      "metadata": {
        "id": "h37xxhJLhvl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "# -----------------------------\n",
        "# Load OpenAI key from HF Secrets\n",
        "# -----------------------------\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\n",
        "        \"OPENAI_API_KEY is not set. \"\n",
        "        \"Add it in your Hugging Face Space: Settings → Variables and secrets → Secrets.\"\n",
        "    )\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"Create an intelligent Python chatbot capable of engaging in natural, helpful, and contextually appropriate conversations with human users.\n",
        "\n",
        "Requirements:\n",
        "- Maintain conversational context over multiple user turns.\n",
        "- Respond helpfully and accurately to a wide range of user inputs.\n",
        "- Reason about user intent before generating each response.\n",
        "- Politely ask clarifying questions if a request is ambiguous or unclear.\n",
        "- Avoid hallucination or speculation—respond only with information you can justify or infer from context.\n",
        "- If unable to answer, politely acknowledge the limitation.\n",
        "\n",
        "Process:\n",
        "1. On each user message, first analyze prior context (if any) and what the user is likely asking/intending.\n",
        "2. Think step-by-step (chain-of-thought) to determine the most relevant, helpful response. Always reason internally before presenting your answer.\n",
        "3. If more information is needed, ask targeted clarifying questions.\n",
        "4. Output your response, maintaining natural tone and conversational flow.\n",
        "5. Continue the conversation until the user indicates they are finished.\n",
        "\n",
        "Output:\n",
        "- Each response should be in plain English, no markdown or code blocks unless explicitly requested.\n",
        "- Maintain a single-paragraph, natural-sounding chat response of 1–3 sentences (unless a longer reply is requested or required).\n",
        "\n",
        "Example—Instructions:\n",
        "- Reasoning: \"Recognize the user asked for Python list examples and may want to know how lists work.\"\n",
        "- Conclusion/Output: \"Sure! In Python, a list is a collection of items in a particular order. For example: my_list = [1, 2, 3, 4]. Would you like to see how to add or remove items?\"\n",
        "\n",
        "(For more advanced technical requests, reasoning steps and explanations may be slightly longer, but always conclude with a concise, clear reply to the user.)\n",
        "\n",
        "Edge Cases & Important Considerations:\n",
        "- If the user refers to prior conversation context, recall and incorporate it.\n",
        "- Be warm, engaging, and never condescending.\n",
        "- If asked for code, provide only what is needed and explain concisely.\n",
        "\n",
        "REMINDER: Your primary objective is to serve as a helpful Python chatbot, reasoning about context before each response, and outputting clear, appropriate conversational replies.\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------\n",
        "# OpenAI message state (internal)\n",
        "# -----------------------------\n",
        "def init_messages():\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_PROMPT}]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio Chatbot UI history (messages format)\n",
        "# Each item must be: {\"role\": \"...\", \"content\": \"...\"}\n",
        "# -----------------------------\n",
        "def append_ui_history(chat_history, user_text, assistant_text):\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "    chat_history = chat_history + [\n",
        "        {\"role\": \"user\", \"content\": user_text},\n",
        "        {\"role\": \"assistant\", \"content\": assistant_text},\n",
        "    ]\n",
        "    return chat_history\n",
        "\n",
        "def respond(user_text, chat_history, messages):\n",
        "    if messages is None:\n",
        "        messages = init_messages()\n",
        "\n",
        "    # add user turn for OpenAI\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"input_text\", \"text\": user_text}]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # call API\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-5-chat-latest\",\n",
        "        input=messages,\n",
        "        text={\"format\": {\"type\": \"text\"}},\n",
        "        reasoning={},\n",
        "        tools=[],\n",
        "        temperature=1,\n",
        "        max_output_tokens=2048,\n",
        "        top_p=1,\n",
        "        store=True\n",
        "    )\n",
        "\n",
        "    assistant_text = response.output_text\n",
        "\n",
        "    # add assistant turn for OpenAI\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"output_text\", \"text\": assistant_text}]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # update UI history (messages format)\n",
        "    chat_history = append_ui_history(chat_history, user_text, assistant_text)\n",
        "\n",
        "    return \"\", chat_history, messages\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio UI\n",
        "# -----------------------------\n",
        "FAQ_QUESTIONS = [\n",
        "    \"What is the difference between a list, tuple, and set in Python?\",\n",
        "    \"How do I use dictionaries effectively in Python?\",\n",
        "    \"What are Python functions and how do *args and **kwargs work?\",\n",
        "    \"How does OOP work in Python (classes, objects, inheritance)?\",\n",
        "    \"How do I handle errors using try/except?\",\n",
        "    \"What are list comprehensions and when should I use them?\",\n",
        "    \"How do I read and write files in Python?\"\n",
        "]\n",
        "\n",
        "def set_question(q):\n",
        "    return q\n",
        "\n",
        "def clear_all():\n",
        "    return [], init_messages(), \"\"\n",
        "\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/Decoding-Data-Science/nov25/main/logo_python.png\"\n",
        "\n",
        "css = \"\"\"\n",
        "#app_container {max-width: 1200px; margin: 0 auto;}\n",
        "\n",
        ".header-wrap {\n",
        "    display: flex;\n",
        "    align-items: center;\n",
        "    gap: 14px;\n",
        "    padding: 10px 6px 2px 6px;\n",
        "}\n",
        ".header-title {\n",
        "    font-size: 28px;\n",
        "    font-weight: 700;\n",
        "    line-height: 1.1;\n",
        "}\n",
        ".header-subtitle {\n",
        "    font-size: 12.5px;\n",
        "    opacity: 0.75;\n",
        "    margin-top: 2px;\n",
        "}\n",
        "\n",
        ".faq-box {\n",
        "    border: 1px solid rgba(255,255,255,0.08);\n",
        "    border-radius: 12px;\n",
        "    padding: 14px;\n",
        "}\n",
        "\n",
        ".faq-btn button {\n",
        "    width: 100%;\n",
        "    justify-content: flex-start;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(elem_id=\"app_container\") as demo:\n",
        "    # Header Row\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1, min_width=80):\n",
        "            gr.Image(\n",
        "                value=LOGO_URL,\n",
        "                label=None,\n",
        "                show_label=False,\n",
        "                height=64,\n",
        "                width=64,\n",
        "                container=False\n",
        "            )\n",
        "        with gr.Column(scale=10):\n",
        "            gr.HTML(\n",
        "                \"\"\"\n",
        "                <div class=\"header-wrap\">\n",
        "                    <div>\n",
        "                        <div class=\"header-title\">Python Tutor Bot</div>\n",
        "                        <div class=\"header-subtitle\">\n",
        "                            Ask anything about Python — concepts, debugging, best practices, and examples.\n",
        "                        </div>\n",
        "                    </div>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "            )\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "\n",
        "    # State for OpenAI messages\n",
        "    state = gr.State(init_messages())\n",
        "\n",
        "    # Two-column layout\n",
        "    with gr.Row(equal_height=True):\n",
        "        # LEFT: FAQ + Quick Ask\n",
        "        with gr.Column(scale=4, min_width=320):\n",
        "            with gr.Group(elem_classes=[\"faq-box\"]):\n",
        "                gr.Markdown(\"### FAQ — Most Asked Python Questions\")\n",
        "                gr.Markdown(\"Click a question to auto-fill it, then press **Enter** or click **Send**.\")\n",
        "\n",
        "                faq_buttons = []\n",
        "                for q in FAQ_QUESTIONS:\n",
        "                    b = gr.Button(q, elem_classes=[\"faq-btn\"])\n",
        "                    faq_buttons.append(b)\n",
        "\n",
        "                gr.Markdown(\"### Quick prompt ideas\")\n",
        "                quick = gr.Radio(\n",
        "                    choices=[\n",
        "                        \"Explain with a simple example\",\n",
        "                        \"Give me a beginner-friendly analogy\",\n",
        "                        \"Show common mistakes to avoid\",\n",
        "                        \"Provide a short quiz question\",\n",
        "                        \"Compare two approaches briefly\"\n",
        "                    ],\n",
        "                    label=\"Add a style preference (optional)\",\n",
        "                    value=None\n",
        "                )\n",
        "\n",
        "        # RIGHT: Chat area\n",
        "        with gr.Column(scale=8, min_width=520):\n",
        "            chatbot = gr.Chatbot(\n",
        "                height=520,\n",
        "                label=\"Conversation\"\n",
        "                # No bubble_full_width\n",
        "                # No type=\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                msg = gr.Textbox(\n",
        "                    placeholder=\"Type your Python question here…\",\n",
        "                    label=None,\n",
        "                    scale=9\n",
        "                )\n",
        "                send = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "            with gr.Row():\n",
        "                clear = gr.Button(\"Clear Chat\")\n",
        "                gr.Markdown(\n",
        "                    \"<span style='opacity:0.7;font-size:12px;'>Context is preserved across turns unless you clear.</span>\"\n",
        "                )\n",
        "\n",
        "    # FAQ -> fill textbox\n",
        "    for b, q in zip(faq_buttons, FAQ_QUESTIONS):\n",
        "        b.click(fn=lambda q=q: set_question(q), inputs=None, outputs=msg)\n",
        "\n",
        "    # Optional quick preference: append hint to textbox (UI-only)\n",
        "    def apply_quick_pref(pref, current_text):\n",
        "        if not pref:\n",
        "            return current_text\n",
        "        if current_text and current_text.strip():\n",
        "            return f\"{current_text.strip()} ({pref})\"\n",
        "        return pref\n",
        "\n",
        "    quick.change(fn=apply_quick_pref, inputs=[quick, msg], outputs=msg)\n",
        "\n",
        "    # Submit logic\n",
        "    msg.submit(respond, inputs=[msg, chatbot, state], outputs=[msg, chatbot, state])\n",
        "    send.click(respond, inputs=[msg, chatbot, state], outputs=[msg, chatbot, state])\n",
        "\n",
        "    # Clear\n",
        "    clear.click(fn=clear_all, inputs=None, outputs=[chatbot, state, msg])\n",
        "\n",
        "demo.launch(\n",
        "    debug=False,\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=css\n",
        ")\n"
      ],
      "metadata": {
        "id": "ETjWIY-PheDe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}